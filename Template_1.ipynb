{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "insured-junior",
   "metadata": {},
   "source": [
    "# Template to test methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "protective-bhutan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-spell",
   "metadata": {},
   "source": [
    "## A. Load Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "grand-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from implementations import *\n",
    "SET_TRAIN = 'data/train.csv' # download train data\n",
    "y, tx, ids = load_csv_data(SET_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "durable-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize data\n",
    "tx, x_mean, x_std = standardize(tx)\n",
    "y, y_mean, y_std = standardize(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "accomplished-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce outliers\n",
    "for num, row in enumerate(tx.T):\n",
    "    tx.T[num] = reduce_outliers(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-israel",
   "metadata": {},
   "source": [
    "## B. Train data with different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "finished-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-mayor",
   "metadata": {},
   "source": [
    "### 1] Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "coastal-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30) (250000,)\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.ones(tx.shape[1])\n",
    "print(tx.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "humanitarian-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD: execution time=0.772 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start gradient descent.\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "gd_w, gd_loss = mean_squared_error_gd(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "tired-endorsement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4614137150198522 1.200146801660332\n"
     ]
    }
   ],
   "source": [
    "print(tx[0,0], gd_loss)\n",
    "\n",
    "weights.append(gd_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "massive-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731725908292163\n"
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "\n",
    "\n",
    "y, tx, ids = load_csv_data(\"data/train.csv\")\n",
    "\n",
    "for num, row in enumerate(tx.T):\n",
    "    tx.T[num] = reduce_outliers(row)\n",
    "\n",
    "w = np.ones(30)\n",
    "\n",
    "\n",
    "#print(x_clean)\n",
    "#x_clean = (np.abs(tx) > mean_tx + 2 * std_tx)\n",
    "# print(x_clean)\n",
    "w, loss = mean_squared_error_gd(y, tx, w, 100, 0.01)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-minority",
   "metadata": {},
   "source": [
    "### 2] Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "foreign-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.ones(tx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "everyday-airplane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: execution time=27.316 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_w, sgd_loss = mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "three-treasurer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4333147897529523\n"
     ]
    }
   ],
   "source": [
    "print(sgd_loss)\n",
    "\n",
    "weights.append(sgd_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-colon",
   "metadata": {},
   "source": [
    "### 3] Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bright-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Squares: execution time=0.104 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Least_Squares.\n",
    "start_time = datetime.datetime.now()\n",
    "ls_w, ls_loss = least_squares(y, tx)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Least Squares: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fuzzy-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38977101340468845\n"
     ]
    }
   ],
   "source": [
    "print(ls_loss)\n",
    "\n",
    "weights.append(ls_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-prince",
   "metadata": {},
   "source": [
    "### 4] Ridge Regressionlambdas = np.logspace(-5, 0, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "stuck-concrete",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.001, Training RMSE=0.390\n",
      "lambda=0.002, Training RMSE=0.390\n",
      "lambda=0.003, Training RMSE=0.390\n",
      "lambda=0.004, Training RMSE=0.390\n",
      "lambda=0.007, Training RMSE=0.391\n",
      "lambda=0.012, Training RMSE=0.391\n",
      "lambda=0.019, Training RMSE=0.392\n",
      "lambda=0.032, Training RMSE=0.394\n",
      "lambda=0.052, Training RMSE=0.397\n",
      "lambda=0.085, Training RMSE=0.401\n",
      "lambda=0.139, Training RMSE=0.407\n",
      "lambda=0.228, Training RMSE=0.412\n",
      "lambda=0.373, Training RMSE=0.419\n",
      "lambda=0.611, Training RMSE=0.427\n",
      "lambda=1.000, Training RMSE=0.436\n"
     ]
    }
   ],
   "source": [
    "lambdas = np.logspace(-3, 0, 15)\n",
    "losses = []\n",
    "\n",
    "for ind, lambda_ in enumerate(lambdas):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression with a given lambda\n",
    "    # ***************************************************\n",
    "    w, loss = ridge_regression(y, tx, lambda_)\n",
    "    losses.append(loss)\n",
    "    #rmse_te.append(np.sqrt(2 * compute_mse(y_te, tx_te, w)))\n",
    "    print(\"lambda={l:.3f}, Training RMSE={tr:.3f}\".format(l=lambda_, tr=losses[ind]))\n",
    "#plot_train_test(losses, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "sunrise-oxford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression: execution time=0.103 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Ridge Regression.\n",
    "start_time = datetime.datetime.now()\n",
    "rr_w, rr_loss = ridge_regression(y, tx, lambdas[0])\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Ridge Regression: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "weights.append(rr_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-supervisor",
   "metadata": {},
   "source": [
    "### 5] Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "toxic-diameter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250000\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.ones(tx.shape[1])\n",
    "\n",
    "print(y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bulgarian-saudi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=nan\n",
      "Current iteration=200, loss=nan\n",
      "Current iteration=300, loss=nan\n",
      "Current iteration=400, loss=nan\n",
      "Current iteration=500, loss=nan\n",
      "Current iteration=600, loss=nan\n",
      "Current iteration=700, loss=nan\n",
      "Current iteration=800, loss=nan\n",
      "Current iteration=900, loss=nan\n",
      "Logistic Regression: execution time=50.068 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Logistic Regression.\n",
    "start_time = datetime.datetime.now()\n",
    "lr_w, lr_loss = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Logistic Regression: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "weights.append(lr_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-bottle",
   "metadata": {},
   "source": [
    "### 6] Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "animal-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.ones(tx.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "political-administration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=0.5890728068128798\n",
      "Current iteration=200, loss=0.5523551972844616\n",
      "Current iteration=300, loss=0.5516219002922558\n",
      "Current iteration=400, loss=0.5515792792902421\n",
      "Logistic Regression: execution time=20.573 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Regularized Logistic Regression.\n",
    "start_time = datetime.datetime.now()\n",
    "rlr_w, rlr_loss = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Logistic Regression: execution time={t:.3f} seconds\".format(t=exection_time))\n",
    "\n",
    "weights.append(rlr_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-ebony",
   "metadata": {},
   "source": [
    "## C. Submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "knowing-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_TEST = 'data/test.csv' # TODO: download train data and supply path here \n",
    "_, tx_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "characteristic-subdivision",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-8b96153d5827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mOUTPUT\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'output/out.csv'\u001b[0m \u001b[0;31m# TODO: fill in desired name of output file for submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcreate_csv_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "OUTPUT= 'output/out.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tx_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-scratch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
